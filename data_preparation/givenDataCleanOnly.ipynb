{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pzZIhdxnoomo",
   "metadata": {
    "id": "pzZIhdxnoomo"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gZLOrUCnowyg",
   "metadata": {
    "id": "gZLOrUCnowyg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "spark_url = 'local'\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MVq0mm7KoyBk",
   "metadata": {
    "id": "MVq0mm7KoyBk"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark Data Prep')\\\n",
    "        .config('spark.ui.port', '4040')\\\n",
    "        .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"16g\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path = 'bangkok_traffy.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50770241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891b573",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc85973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f08aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ticket_id', 'organization','timestamp','last_activity','state','coords', 'type'], how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0e33c",
   "metadata": {},
   "source": [
    "## preprocessing state and completion time calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a6ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col('state')==\"เสร็จสิ้น\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b606e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('completion_time_hours', \n",
    "                   (F.unix_timestamp('last_activity') - F.unix_timestamp('timestamp')) / 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893ad93",
   "metadata": {},
   "source": [
    "## preprocessing coordinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1bcfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'coords' column into two new columns 'latitude' and 'longitude'\n",
    "df = df.withColumn('longitude', split(df['coords'], ',').getItem(0).cast('double')) \\\n",
    "       .withColumn('latitude', split(df['coords'], ',').getItem(1).cast('double'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524917e5",
   "metadata": {},
   "source": [
    "## deal with out of range coordination (outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7cc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the latitude and longitude bounds mainly in กรุงเทพ\n",
    "latitude_min = 13.4\n",
    "latitude_max = 14.3\n",
    "longitude_min = 99.8\n",
    "longitude_max = 101\n",
    "\n",
    "# keep only the rows within the defined bounds\n",
    "df = df.filter(\n",
    "    (df['latitude'] >= latitude_min) & (df['latitude'] <= latitude_max) &\n",
    "    (df['longitude'] >= longitude_min) & (df['longitude'] <= longitude_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cc645",
   "metadata": {},
   "source": [
    "## preprocessing province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54674b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows that province are กรุงเทพ และ ปริมณฑล\n",
    "pattern = \"(?i)กรุงเทพ|Bangkok|นนทบุรี|นครปฐม|ปทุมธานี|สมุทรปราการ|สมุทรสาคร\"\n",
    "df = df.filter(col(\"province\").isNotNull() & col(\"province\").rlike(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8bcbd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pattern matching and replace values\n",
    "df = df.withColumn(\n",
    "    \"province\",\n",
    "    F.when(\n",
    "        F.col(\"province\").rlike(\"(?i).*กรุงเทพ.*|.*Bangkok.*\"), \"กรุงเทพมหานคร\"\n",
    "    )\n",
    "    .when(F.col(\"province\").rlike(\"(?i).*นนทบุรี.*\"), \"นนทบุรี\")\n",
    "    .when(F.col(\"province\").rlike(\"(?i).*สมุทรปราการ.*\"), \"สมุทรปราการ\")\n",
    "    .when(F.col(\"province\").rlike(\"(?i).*ปทุมธานี.*\"), \"ปทุมธานี\")\n",
    "    .when(F.col(\"province\").rlike(\"(?i).*สมุทรสาคร.*\"), \"สมุทรสาคร\")\n",
    "    .when(F.col(\"province\").rlike(\"(?i).*นครปฐม.*\"), \"นครปฐม\")\n",
    "    .otherwise(F.col(\"province\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5dbfd1",
   "metadata": {},
   "source": [
    "## preprocessing organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "272b1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'organization' to array\n",
    "df = df.withColumn('organization_array', split(F.regexp_replace(F.col('organization'), '\\\\s*,\\\\s*', ','), ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f2246b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove organization from the array that look like person names in parentheses, e.g. \"... (นาง...)\"/\"... (นาย...)\"\n",
    "person_pattern = r'\\( *?(นาย|นาง|น\\.ส\\.|นางสาว)[^\\)]*\\)'\n",
    "\n",
    "# trim items, remove empties and remove items matching the person pattern\n",
    "df = df.withColumn(\n",
    "    \"organization_array\",\n",
    "    F.filter(\n",
    "        F.transform(F.col(\"organization_array\"), lambda x: F.trim(x)),\n",
    "        lambda x: (x != \"\") & (~F.lower(x).rlike(person_pattern))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d35a4a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- organization_raw: string (nullable = true)\n",
      " |-- displayName: string (nullable = true)\n",
      " |-- formattedAddress: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pathOrgsLoc = 'org_with_loc_v2.csv'\n",
    "dfOrg = spark.read.csv(pathOrgsLoc, header=True, inferSchema=True)\n",
    "dfOrg.printSchema()\n",
    "\n",
    "# prepare org location dictionary for fast lookup\n",
    "org_loc = {row['organization_raw']: (row['latitude'], row['longitude']) for row in dfOrg.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "373d453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "# only keep organizations that have valid locations in the org_loc dictionary\n",
    "def filter_orgs(orgs):\n",
    "    filtered = []\n",
    "    if orgs is None:\n",
    "        return filtered\n",
    "    for org in orgs:\n",
    "        loc = org_loc.get(org)\n",
    "        if loc and loc[0] != \"Not Found\" and loc[1] != \"Not Found\":\n",
    "            filtered.append(org)\n",
    "    return filtered\n",
    "\n",
    "filter_orgs_udf = F.udf(filter_orgs, ArrayType(StringType()))\n",
    "\n",
    "# apply filter to the array\n",
    "df = df.withColumn('organization_array', filter_orgs_udf(col('organization_array')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae4574",
   "metadata": {},
   "source": [
    "## calculate orgs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daee25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import math\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# calculate haversine distance between two points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_distances(orgs, lat, lon):\n",
    "    if orgs is None or lat is None or lon is None:\n",
    "        return []\n",
    "    distances = set()\n",
    "    for org in orgs:\n",
    "        loc = org_loc.get(org)\n",
    "        if loc and loc[0] != \"Not Found\" and loc[1] != \"Not Found\":\n",
    "            try:\n",
    "                dist = haversine(lat, lon, float(loc[0]), float(loc[1]))\n",
    "                distances.add(dist)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return list(distances)\n",
    "\n",
    "calc_distances_udf = udf(calc_distances, ArrayType(DoubleType()))\n",
    "\n",
    "# calculate distances from ticket latitude and longitude to each organization's location\n",
    "df = df.withColumn(\n",
    "    'organization_distances',\n",
    "    calc_distances_udf(col('organization_array'), col('latitude'), col('longitude'))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e122879",
   "metadata": {},
   "source": [
    "## preprocessing type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d4700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty type\n",
    "df = df.filter(df['type'] != '{}')\n",
    "# Remove '{' and '}' and split the 'type' column by ','\n",
    "df = df.withColumn('type_array', F.split(F.regexp_replace(df['type'], '[\\{\\}]', ''), ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3f7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# a row will will get exploded into multiple rows, one for each type in the type_array\n",
    "df_exploded = df.withColumn(\"type_array\", explode(df[\"type_array\"]))\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"type_array\", outputCol=\"type_index\")\n",
    "df_indexed = indexer.fit(df_exploded).transform(df_exploded)\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"type_index\", outputCol=\"type_onehot\")\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e9ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregate one-hot encoded vectors for each ID\n",
    "df_combined = df_encoded.groupBy(\"ticket_id\").agg(\n",
    "    F.collect_list(\"type_onehot\").alias(\"type_onehots\")\n",
    ")\n",
    "\n",
    "df = df.join(df_combined, on='ticket_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33dd40",
   "metadata": {},
   "source": [
    "# Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a02a4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+----+------------+-------------+---------------------+---------+--------+------------------+----------------------+----------+------------+\n",
      "|ticket_id|type|organization|comment|photo|photo_after|coords|address|subdistrict|district|province|timestamp|state|star|count_reopen|last_activity|completion_time_hours|longitude|latitude|organization_array|organization_distances|type_array|type_onehots|\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+----+------------+-------------+---------------------+---------+--------+------------------+----------------------+----------+------------+\n",
      "+---------+----+------------+-------+-----+-----------+------+-------+-----------+--------+--------+---------+-----+----+------------+-------------+---------------------+---------+--------+------------------+----------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.organization == \"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "147afd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544599"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b36a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticket_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- photo: string (nullable = true)\n",
      " |-- photo_after: string (nullable = true)\n",
      " |-- coords: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- subdistrict: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- star: integer (nullable = true)\n",
      " |-- count_reopen: integer (nullable = true)\n",
      " |-- last_activity: timestamp (nullable = true)\n",
      " |-- completion_time_hours: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- organization_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- organization_distances: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- type_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- type_onehots: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "601aaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProcessed = df.select(df.latitude, df.longitude, df.organization_array, df.organization_distances, df.type_array, df.type_onehots, df.completion_time_hours)\n",
    "train_data, test_data = dfProcessed.randomSplit([0.8, 0.2], seed=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProcessed.coalesce(1).write.mode(\"overwrite\").json('file:///C:/Users/Noon/Documents/DSDE/projectTraffy/raw_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.coalesce(1).write.mode(\"overwrite\").json('file:///C:/Users/Noon/Documents/DSDE/projectTraffy/train_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.coalesce(1).write.mode(\"overwrite\").json('file:///C:/Users/Noon/Documents/DSDE/projectTraffy/test_data.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
